{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb2b4e8-159c-4825-834b-4378d2ef6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import evaluate\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, get_scheduler\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")\n",
    "\n",
    "# huggingface cache\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"D:/huggingface_cache\"\n",
    "os.environ[\"HF_DATASET_CACHE\"] = \"D:/huggingface_cache/datasets\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/huggingface_cache/models\"\n",
    "\n",
    "# PyTorch cache\n",
    "os.environ[\"TORCH_HOME\"] = \"D:/torch_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d53dee5-98ff-4aba-b3f6-8ec71b47bc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# loading gpt2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "max_steps = 50000\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e60616-e383-45d5-a574-17fcb3b21d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading wikitext dataset\n",
    "dataset = load_dataset(\"lfsm/multimodal_wiki\", split=\"train\", streaming=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ca50cf-3764-4781-a8a5-89be1cdcf3e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m             encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m {\n\u001b[0;32m     11\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     12\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     13\u001b[0m             }\n\u001b[1;32m---> 14\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m Dataset(\u001b[43mtrain_stream\u001b[49m, tokenizer)\n\u001b[0;32m     15\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m Dataset(eval_stream, tokenizer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_stream' is not defined"
     ]
    }
   ],
   "source": [
    "# dataset loading class\n",
    "class Dataset(IterableDataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __iter__(self):\n",
    "        for example in self.dataset:\n",
    "            encoding = self.tokenizer(example[\"text\"], truncation=True, return_tensors=\"pt\")\n",
    "            yield {\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ef8623-f69d-40a6-b18a-b695d742e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap dataset in PyTorch Dataset class\n",
    "custom_dataset = Dataset(dataset, tokenizer)\n",
    "train_dataloader = DataLoader(custom_dataset, batch_size=4, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f901e947-78bf-4a9b-a91f-b373a04c9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss_and_accuracy(model, eval_dataset) -> tuple[float, float]:\n",
    "\n",
    "    # Estimate the loss and accuracy of a model on an evaluation dataset.\n",
    "\n",
    "    model.eval()  # evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # compute loss with gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            inputs = batch[\"input_ids\"].to(model.device)\n",
    "            labels = inputs.clone()\n",
    "\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            mask = labels != -100  # assumes masked labels\n",
    "            correct = ((predictions == labels) & mask).sum().item()\n",
    "            total_correct += correct\n",
    "            total_tokens += mask.sum().item()\n",
    "            \n",
    "            num_batches += 1\n",
    "\n",
    "    model.train()  # training mode\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else float(\"inf\")\n",
    "    accuracy = total_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192413cd-676b-46d8-83fa-8a079672a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize dataset\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2ed9339-4810-4cab-8155-c70571fd0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"F:/GPT2_Tuning/training_output\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"F:/GPT2_Tuning/logs\",\n",
    "    logging_steps=1000,\n",
    "    save_steps=2000,\n",
    "    warmup_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    max_steps=max_steps,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ce4eff-5168-4be8-93cb-645767ab8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1000,\n",
    "    num_training_steps=max_steps\n",
    ")\n",
    "\n",
    "# Load metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Convert logits → predicted token IDs\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    return torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Compute accuracy\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    if predictions.shape != labels.shape:\n",
    "        labels = labels[:, :predictions.shape[1]]\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "861fef52-87d4-4b14-9910-ae88130723c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# initialize trainer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m----> 5\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_dataset\u001b[49m,\n\u001b[0;32m      6\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[0;32m      7\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m      8\u001b[0m     preprocess_logits_for_metrics\u001b[38;5;241m=\u001b[39mpreprocess_logits_for_metrics,\n\u001b[0;32m      9\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     10\u001b[0m     optimizers\u001b[38;5;241m=\u001b[39m(optimizer, lr_scheduler)\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Start training and estimate loss after each epoch\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(training_args\u001b[38;5;241m.\u001b[39mnum_train_epochs):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, lr_scheduler)\n",
    ")\n",
    "\n",
    "# Start training and estimate loss after each epoch\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{training_args.num_train_epochs}\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Estimate loss\n",
    "    eval_loss = estimate_loss_and_accuracy(model, tokenized_datasets[\"train\"])\n",
    "    print(f\"Estimated Loss after Epoch {epoch+1}: {eval_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee6c9e-ad50-4696-bb77-f287dde47999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity from eval loss\n",
    "eval_results = trainer.evaluate()\n",
    "eval_loss = eval_results[\"eval_loss\"]\n",
    "perplexity = math.exp(eval_loss)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "# Generate text from a prompt\n",
    "prompt = \"In a distant future, humanity has\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated Text:\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66ee7c7f-cbde-4a18-a61b-d4c3ff249045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (Type 'exit' to quit):  hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot Response: hello. I’m a big fan of the old school, classic and modern style.. My favorite is that you can wear it with jeans or t-shirts for your summer party!. And my favourite part about this dress? It has an ankle strap to hold everything in place so no need getting tangled up on stuff! The straps are made from super soft cotton fabric which makes them perfect as long sleeves. They also come complete w/stitched seams so they don't get messy when worn.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (Type 'exit' to quit):  what dress are you talking about?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot Response: what dress are you talking about?. I’ve been a fan of the classic, but now it is time to change. This week we have our first look at this gorgeous new collection from The Dormouse.. It was inspired by my favorite book “The Little Mermaid: A Memoirs Of My Life and Years With Love (1944-1949) – which has become one among many books that inspire me today! You can see more images in these gallery below!. Here\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (Type 'exit' to quit):  what dress are you going to wear?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot Response: what dress are you going to wear?. Dress up your wedding with a matching outfit for the perfect look.. This is an easy way of showing off that style and make it more memorable. You can choose from different styles, such as black or white dresses which will be worn with all kinds accessories including flowers in front!. The best part about this piece though – it's made outof cotton so if there’s anything else on hand then we don't have any choice but do our own research\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (Type 'exit' to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot session ended.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"F:/GPT2_Tuning/gpt2_training_checkpoints/checkpoint-50000\"\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def generate_response(prompt, max_length=100):\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_length=max_length,\n",
    "            temperature=0.3,\n",
    "            top_k=50,         # Limits to top-k words\n",
    "            top_p=0.95,       # Nucleus sampling\n",
    "            repetition_penalty=1.8,  # Prevents repetitive outputs\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the model\n",
    "while True:\n",
    "    prompt = input(\"Enter your question (Type 'exit' to quit): \")\n",
    "    \n",
    "    if prompt.lower() == \"exit\":\n",
    "        print(\"Chatbot session ended.\")\n",
    "        break\n",
    "    \n",
    "    response = generate_response(prompt)\n",
    "    print(\"Chatbot Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12edff28-c54c-4f68-9ac3-d1636bc26fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gopu",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
