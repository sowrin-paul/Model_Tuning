{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb2b4e8-159c-4825-834b-4378d2ef6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import evaluate\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, get_scheduler\n",
    "from huggingface_hub import login\n",
    "# If you encounter ModuleNotFoundError for '_bz2', you need to install the bzip2 system library.\n",
    "# For Ubuntu/Debian: sudo apt-get install libbz2-dev\n",
    "# For MacOS (Homebrew): brew install bzip2\n",
    "# After installing, you may need to reinstall Python or rebuild it with bzip2 support.\n",
    "# If using conda: conda install bzip2\n",
    "# Then reinstall the affected Python packages if needed.\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d53dee5-98ff-4aba-b3f6-8ec71b47bc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training configuration:\n",
      "Max length: 256\n",
      "Max steps: 1200\n",
      "Learning rate: 0.0001\n",
      "Batch size: 8\n",
      "Epochs: 10\n",
      "Weight decay: 0.1\n",
      "Warmup steps: 120\n"
     ]
    }
   ],
   "source": [
    "# loading gpt2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training parameters\n",
    "max_length = 256\n",
    "max_steps = 1200\n",
    "learning_rate = 1e-4\n",
    "batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 10\n",
    "weight_decay = 0.1\n",
    "warmup_steps = 120\n",
    "save_steps = 200\n",
    "logging_steps = 50\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"Max length: {max_length}\")\n",
    "print(f\"Max steps: {max_steps}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Epochs: {num_train_epochs}\")\n",
    "print(f\"Weight decay: {weight_decay}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e60616-e383-45d5-a574-17fcb3b21d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 600 conversations\n",
      "Sample conversation:\n",
      "User: How does your relationship with your mother affect your autonomy?\n",
      "Assistant: In this case, the respondent scored 30, 27, and 32 for mother-related items, and 27 for autonomy. This suggests that ...\n",
      "Training samples: 480\n",
      "Validation samples: 120\n"
     ]
    }
   ],
   "source": [
    "# Loading adolescent chatbot dataset\n",
    "def load_adolescent_chatbot_dataset(file_path):\n",
    "    \"\"\"Load and format the adolescent chatbot dataset\"\"\"\n",
    "    conversations = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            # Format as conversation: user message + assistant response\n",
    "            conversation = \"\"\n",
    "            for message in data['messages']:\n",
    "                if message['role'] == 'user':\n",
    "                    conversation += f\"User: {message['content']}\\n\"\n",
    "                elif message['role'] == 'assistant':\n",
    "                    conversation += f\"Assistant: {message['content']}\\n\"\n",
    "            conversations.append(conversation.strip())\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"adolescent_chatbot_train.jsonl\"\n",
    "conversations = load_adolescent_chatbot_dataset(dataset_path)\n",
    "\n",
    "print(f\"Loaded {len(conversations)} conversations\")\n",
    "print(f\"Sample conversation:\\n{conversations[0][:200]}...\")\n",
    "\n",
    "# Split into train/validation (80/20)\n",
    "train_size = int(0.8 * len(conversations))\n",
    "train_conversations = conversations[:train_size]\n",
    "val_conversations = conversations[train_size:]\n",
    "\n",
    "print(f\"Training samples: {len(train_conversations)}\")\n",
    "print(f\"Validation samples: {len(val_conversations)}\")\n",
    "\n",
    "# Convert to HuggingFace Dataset format\n",
    "train_dataset = HFDataset.from_dict({\"text\": train_conversations})\n",
    "val_dataset = HFDataset.from_dict({\"text\": val_conversations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ca50cf-3764-4781-a8a5-89be1cdcf3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 480\n",
      "Validation dataset size: 120\n",
      "Sample input_ids shape: torch.Size([256])\n",
      "Sample attention_mask shape: torch.Size([256])\n",
      "Sample text: User: How does your relationship with your mother affect your autonomy?\n",
      "Assistant: In this case, the respondent scored 30, 27, and 32 for mother-related items, and 27 for autonomy. This suggests that perceived maternal support or control may be\n"
     ]
    }
   ],
   "source": [
    "# Dataset class for adolescent chatbot training\n",
    "class AdolescentChatbotDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=256):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx][\"text\"]\n",
    "\n",
    "        # tokenize dataset with proper padding and truncation\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# dataset instances\n",
    "train_dataset_custom = AdolescentChatbotDataset(train_dataset, tokenizer, max_length)\n",
    "val_dataset_custom = AdolescentChatbotDataset(val_dataset, tokenizer, max_length)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset_custom)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset_custom)}\")\n",
    "\n",
    "# tokenization\n",
    "sample = train_dataset_custom[0]\n",
    "print(f\"Sample input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Sample attention_mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Sample text: {tokenizer.decode(sample['input_ids'][:50])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ef8623-f69d-40a6-b18a-b695d742e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 60\n",
      "Validation batches: 15\n",
      "Batch input_ids shape: torch.Size([8, 256])\n",
      "Batch attention_mask shape: torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "# DataLoaders for training and validation\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset_custom,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if device == \"cuda\" else False\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset_custom,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if device == \"cuda\" else False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Validation batches: {len(val_dataloader)}\")\n",
    "\n",
    "# Test batch loading\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"Batch input_ids shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"Batch attention_mask shape: {sample_batch['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f901e947-78bf-4a9b-a91f-b373a04c9873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing evaluation function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial test loss: 10.2425, accuracy: 0.0197\n"
     ]
    }
   ],
   "source": [
    "def estimate_loss_and_accuracy(model, eval_dataloader, device):\n",
    "    # estimate the loss and accuracy of a model on an evaluation dataset.\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = input_ids.clone()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            mask = (labels != tokenizer.pad_token_id) & (attention_mask == 1)\n",
    "            correct = ((predictions == labels) & mask).sum().item()\n",
    "            total_correct += correct\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else float(\"inf\")\n",
    "    accuracy = total_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# evaluation function\n",
    "print(\"Testing evaluation function...\")\n",
    "model_test = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model_test.to(device)\n",
    "test_loss, test_acc = estimate_loss_and_accuracy(model_test, val_dataloader, device)\n",
    "print(f\"Initial test loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "# del model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192413cd-676b-46d8-83fa-8a079672a53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934760ff6dbd46238f6941a0b93949ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996f582d57cd478888d1e680e8b70172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized train dataset: 480 samples\n",
      "Tokenized validation dataset: 120 samples\n",
      "Data collator configured for causal language modeling\n",
      "Adding labels to datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13bb5e0cbad244a98a66aa8c6fa6d79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df6286441b54de5b28bb92691070c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced tokenized train dataset: 480 samples\n",
      "Enhanced tokenized validation dataset: 120 samples\n"
     ]
    }
   ],
   "source": [
    "# tokenization function for the datasets\n",
    "def tokenize_function(examples):\n",
    "    # dataset tokenizaiton for training\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(f\"Tokenized train dataset: {len(tokenized_train)} samples\")\n",
    "print(f\"Tokenized validation dataset: {len(tokenized_val)} samples\")\n",
    "\n",
    "# data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False, # will try the masked modelling later\n",
    "    pad_to_multiple_of=8,  # optimize gopu memory\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(\"Data collator configured for causal language modeling\")\n",
    "\n",
    "# adding labels to the tokenized datasets\n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "print(\"Adding labels to datasets...\")\n",
    "tokenized_train = tokenized_train.map(add_labels, batched=True)\n",
    "tokenized_val = tokenized_val.map(add_labels, batched=True)\n",
    "\n",
    "print(f\"Enhanced tokenized train dataset: {len(tokenized_train)} samples\")\n",
    "print(f\"Enhanced tokenized validation dataset: {len(tokenized_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2ed9339-4810-4cab-8155-c70571fd0dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `include_inputs_for_metrics` is deprecated and will be removed in version 5 of 🤗 Transformers. Please use `include_for_metrics` list argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured:\n",
      "  Output directory: ./gpt2_adolescent_chatbot_train_output\n",
      "  Batch size: 8\n",
      "  Learning rate: 0.0001\n",
      "  Max steps: 1200\n",
      "  Warmup steps: 120\n",
      "  Weight decay: 0.1\n",
      "  FP16: True\n"
     ]
    }
   ],
   "source": [
    "# loading GPT-2 model\n",
    "print(\"Loading GPT-2 model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.to(device)\n",
    "\n",
    "# resize token embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# training arguments for adolescent chatbot dataset\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_adolescent_chatbot_train_output\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    max_steps=max_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_steps=warmup_steps,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=logging_steps,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True if device == \"cuda\" else False,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True if device == \"cuda\" else False,\n",
    "    label_smoothing_factor=0.1,\n",
    "    include_inputs_for_metrics=True,\n",
    "    prediction_loss_only=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  Output directory: {training_args.output_dir}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")\n",
    "print(f\"  Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"  Weight decay: {training_args.weight_decay}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ce4eff-5168-4be8-93cb-645767ab8b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer configured with learning rate: 0.0001\n",
      "Scheduler configured with warmup steps: 120\n",
      "Metrics computation functions configured\n"
     ]
    }
   ],
   "source": [
    "# optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=max_steps\n",
    ")\n",
    "\n",
    "print(f\"Optimizer configured with learning rate: {learning_rate}\")\n",
    "print(f\"Scheduler configured with warmup steps: {warmup_steps}\")\n",
    "\n",
    "# Load accuracy metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# convert logits to predicted token IDs\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"Preprocess logits for metric computation\"\"\"\n",
    "    return torch.argmax(logits, dim=-1)\n",
    "\n",
    "# accuracy and other metrics\n",
    "def compute_metrics(eval_preds):\n",
    "\n",
    "    if hasattr(eval_preds, 'predictions') and hasattr(eval_preds, 'label_ids'):\n",
    "        predictions = eval_preds.predictions\n",
    "        labels = eval_preds.label_ids\n",
    "    elif isinstance(eval_preds, tuple):\n",
    "        if len(eval_preds) == 2:\n",
    "            predictions, labels = eval_preds\n",
    "        elif len(eval_preds) == 3:\n",
    "            predictions, labels, _ = eval_preds\n",
    "        else:\n",
    "            print(f\"Unexpected eval_preds format with {len(eval_preds)} elements\")\n",
    "            predictions = eval_preds[0]\n",
    "            labels = eval_preds[1]\n",
    "    else:\n",
    "        print(f\"eval_preds is not a tuple: {type(eval_preds)}\")\n",
    "        return {\"accuracy\": 0.0, \"num_tokens\": 0}\n",
    "\n",
    "    #----------------------------------------------------------------------\n",
    "    print(f\"Predictions shape: {predictions.shape}, Labels shape: {labels.shape}\")\n",
    "    #----------------------------------------------------------------------\n",
    "\n",
    "    if predictions.shape != labels.shape:\n",
    "        min_length = min(predictions.shape[1], labels.shape[1])\n",
    "        predictions = predictions[:, :min_length]\n",
    "        labels = labels[:, :min_length]\n",
    "\n",
    "    # for easier processing\n",
    "    predictions_flat = predictions.reshape(-1)\n",
    "    labels_flat = labels.reshape(-1)\n",
    "\n",
    "    # creating mask for non-padded tokens\n",
    "    mask = labels_flat != -100\n",
    "\n",
    "    # apply mask\n",
    "    predictions_masked = predictions_flat[mask]\n",
    "    labels_masked = labels_flat[mask]\n",
    "\n",
    "    #----------------------------------------------------------------------\n",
    "    print(f\"Predictions shape: {predictions.shape}, Labels shape: {labels.shape}\")\n",
    "    #----------------------------------------------------------------------\n",
    "\n",
    "    # accuracy calculation\n",
    "    if len(predictions_masked) == 0:\n",
    "        print(\"No valid token found.\")\n",
    "        return {\"accuracy\": 0.0, \"num_tokens\": 0}\n",
    "\n",
    "    # token-level accuracy\n",
    "    correct = (predictions_masked == labels_masked).sum().item() if hasattr(predictions_masked, 'sum') else sum(predictions_masked == labels_masked)\n",
    "    total = len(predictions_masked)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"num_tokens\": total\n",
    "    }\n",
    "\n",
    "print(\"Metrics computation functions configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "861fef52-87d4-4b14-9910-ae88130723c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing trainer...\n",
      "Trainer initialized successfully!\n",
      "Total parameters: 124,439,808\n",
      "Trainable parameters: 124,439,808\n",
      "\n",
      "==================================================\n",
      "STARTING TRAINING\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13083/1357900684.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1149' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1149/1200 05:55 < 00:15, 3.23 it/s, Epoch 19.13/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Num Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.744800</td>\n",
       "      <td>1.628745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.607500</td>\n",
       "      <td>1.584407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.581200</td>\n",
       "      <td>1.576790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.570800</td>\n",
       "      <td>1.567137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.559000</td>\n",
       "      <td>1.565491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.556000</td>\n",
       "      <td>1.556952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.550200</td>\n",
       "      <td>1.560713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.549200</td>\n",
       "      <td>1.560583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.546000</td>\n",
       "      <td>1.561579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.541400</td>\n",
       "      <td>1.562045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.540000</td>\n",
       "      <td>1.563904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n",
      "Predictions shape: (120, 256), Labels shape: (120, 256)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# save the final model\u001b[39;00m\n\u001b[32m     33\u001b[39m     trainer.save_model(\u001b[33m\"\u001b[39m\u001b[33m./gpt2_adolescent_chatbot_final\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model Training/Model_Tuning/env/lib/python3.13/site-packages/transformers/trainer.py:2206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model Training/Model_Tuning/env/lib/python3.13/site-packages/transformers/trainer.py:2502\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2500\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2501\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2502\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2503\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2504\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model Training/Model_Tuning/env/lib/python3.13/site-packages/transformers/trainer.py:5300\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5298\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5299\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5300\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5301\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5302\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model Training/Model_Tuning/env/lib/python3.13/site-packages/accelerate/data_loader.py:577\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    575\u001b[39m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         current_batch = \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m    579\u001b[39m     next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model Training/Model_Tuning/env/lib/python3.13/site-packages/accelerate/utils/operations.py:153\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    151\u001b[39m     device = \u001b[33m\"\u001b[39m\u001b[33mnpu:0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model Training/Model_Tuning/env/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:811\u001b[39m, in \u001b[36mBatchEncoding.to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[32m    807\u001b[39m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[32m    808\u001b[39m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    810\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = {\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m         k: \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[32m    812\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data.items()\n\u001b[32m    813\u001b[39m     }\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    815\u001b[39m     logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "\n",
    "# model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# start training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # train model\n",
    "    trainer.train()\n",
    "\n",
    "    # save the final model\n",
    "    trainer.save_model(\"./gpt2_adolescent_chatbot_final\")\n",
    "    tokenizer.save_pretrained(\"./gpt2_adolescent_chatbot_final\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"\\nRunning final evaluation...\")\n",
    "    final_eval_results = trainer.evaluate()\n",
    "    print(f\"Final evaluation results: {final_eval_results}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee6c9e-ad50-4696-bb77-f287dde47999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation and text generation\n",
    "print(\"Post-training evaluation and testing...\")\n",
    "\n",
    "# perplexity calculation from evaluation loss\n",
    "try:\n",
    "    eval_results = trainer.evaluate()\n",
    "    eval_loss = eval_results[\"eval_loss\"]\n",
    "    perplexity = math.exp(eval_loss)\n",
    "    print(f\"Final Evaluation Loss: {eval_loss:.4f}\")\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")\n",
    "    print(f\"Accuracy: {eval_results.get('eval_accuracy', 'N/A')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed: {e}\")\n",
    "\n",
    "# test text generation with adolescent psychology prompts\n",
    "test_prompts = [\n",
    "    \"User: How does peer pressure affect teenagers?\\nAssistant:\",\n",
    "    \"User: What are the signs of depression in adolescents?\\nAssistant:\",\n",
    "    \"User: How can teenagers manage stress during exams?\\nAssistant:\",\n",
    "    \"User: Why do teenagers take risks?\\nAssistant:\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING TEXT GENERATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # response\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 100,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    print(f\"Generated Response: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee7c7f-cbde-4a18-a61b-d4c3ff249045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chatbot testing\n",
    "print(\"Loading trained model for interactive testing...\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "try:\n",
    "    model_path = \"./gpt2_adolescent_chatbot_final\"\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully!\")\n",
    "except:\n",
    "    print(\"Using current model from training...\")\n",
    "\n",
    "def generate_adolescent_response(user_message, max_length=200, temperature=0.7):\n",
    "    \"\"\"Generate response for adolescent psychology questions\"\"\"\n",
    "    \n",
    "    # Format the input like the training data\n",
    "    prompt = f\"User: {user_message}\\nAssistant:\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "            early_stopping=True,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and extract response\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    # Clean up response (remove extra newlines, etc.)\n",
    "    response = response.split('\\n')[0].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Interactive testing loop\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INTERACTIVE ADOLESCENT PSYCHOLOGY CHATBOT\")\n",
    "print(\"=\"*50)\n",
    "print(\"Ask questions about adolescent psychology. Type 'exit' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['exit', 'quit', 'stop']:\n",
    "            print(\"Chatbot session ended.\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "            \n",
    "        # Generate response\n",
    "        response = generate_adolescent_response(user_input)\n",
    "        print(f\"Assistant: {response}\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nChatbot session interrupted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in chatbot: {e}\")\n",
    "    \n",
    "print(\"Interactive testing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12edff28-c54c-4f68-9ac3-d1636bc26fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
