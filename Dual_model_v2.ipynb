{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2965ca1b",
   "metadata": {},
   "source": [
    "This is a performance testing with the dataset of adolescent on two model. A dual model setup is done here.\n",
    "\n",
    "The first model ***BERT*** is going to train on the emotion and the intent of the user and the ***T5*** model is going to generate text based on the questions.\n",
    "\n",
    "The datasets used here -> **casual_lm_train.jsonl** and **masked_lm_train.jsonl**\n",
    "\n",
    "*casual_lm_train* is used for the **T5** training and the *masked_lm* train is for the **Electra**.\n",
    "\n",
    "*T5*'s performance is going to evaluate by its perplexity that is how certain it's response is or other word what is it's accuracy to predict the next word.\n",
    "\n",
    "*BERT*'s performance is evaluated by it's accuracy to the prediction of the emotion and intention of the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6da7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghost-ed/Documents/Model_Tuning/env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fccaf7c7050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import evaluate\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "bleu_metric = evaluate.load('bleu')\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c4272",
   "metadata": {},
   "source": [
    "**Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec76593",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    jsonl_path = \"/home/ghost-ed/Documents/Model_Tuning/adolescent_chatbot_train.jsonl\"\n",
    "    work_dir = \"/home/ghost-ed/Documents/Model_Tuning\"\n",
    "\n",
    "    # label space\n",
    "    intents = (\n",
    "        \"seek_help\", \"venting\", \"ask_question\", \"share_success\", \"neutral\"\n",
    "    )\n",
    "    emotions = (\n",
    "        \"anxious\", \"sad\", \"angry\", \"lonely\", \"neutral\"\n",
    "    )\n",
    "\n",
    "    clf_model_name = \"bert-base-uncased\"\n",
    "    gen_model_name = \"t5-small\"\n",
    "\n",
    "    clf_epochs = 4\n",
    "    gen_epochs = 4\n",
    "    batch_size = 8\n",
    "    lr = 5e-5\n",
    "    weight_decay = 0.01\n",
    "\n",
    "    # generation config\n",
    "    max_new_token = 96\n",
    "    do_sample = True\n",
    "    temperature = 0.9\n",
    "    top_p = 0.92\n",
    "    repetition_penalty = 1.08\n",
    "\n",
    "\n",
    "CFG = Config()\n",
    "os.makedirs(CFG.work_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f228277",
   "metadata": {},
   "source": [
    "**Dataset loading and heuristic labeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d7a917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: \n",
      "                                                text  \\\n",
      "0  How does your relationship with your mother af...   \n",
      "1  What does a high personal growth score mean du...   \n",
      "2    Why is purpose in life important for teenagers?   \n",
      "3  How does your relationship with your mother af...   \n",
      "4  What does a high personal growth score mean du...   \n",
      "\n",
      "                                            response        intent  emotion  \\\n",
      "0  In this case, the respondent scored 30, 27, an...  ask_question  neutral   \n",
      "1  A personal growth score of 22 indicates that t...  ask_question  neutral   \n",
      "2  In this response, the adolescent scored 27 on ...  ask_question  neutral   \n",
      "3  In this case, the respondent scored 30, 34, an...  ask_question  neutral   \n",
      "4  A personal growth score of 24 indicates that t...  ask_question  neutral   \n",
      "\n",
      "   intent_label  emotion_label  \n",
      "0             2              4  \n",
      "1             2              4  \n",
      "2             2              4  \n",
      "3             2              4  \n",
      "4             2              4  \n",
      "\n",
      "Intent labels:  {0: 'seek_help', 1: 'venting', 2: 'ask_question', 3: 'share_success', 4: 'neutral'}\n",
      "\n",
      "Emotion labels:  {0: 'anxious', 1: 'sad', 2: 'angry', 3: 'lonely', 4: 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl_messages(path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            msgs = obj.get('messages', [])\n",
    "            user = None\n",
    "            assistant = None\n",
    "            for m in msgs:\n",
    "                if m.get('role') == 'user' and user is None:\n",
    "                    user = m.get('content', '').strip()\n",
    "                if m.get('role') == 'assistant' and assistant is None:\n",
    "                    assistant = m.get('content', '').strip()\n",
    "            if user and assistant:\n",
    "                rows.append({\"text\": user, \"response\": assistant})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# intent/emontion heuristics\n",
    "INTENT_PATTERNS = {\n",
    "    \"seek_help\": [r\"help\", r\"what should i\", r\"how do i\", r\"can you\", r\"advice\"],\n",
    "    \"venting\": [r\"no one\", r\"nobody\", r\"so tired\", r\"fed up\", r\"i hate\"],\n",
    "    \"ask_question\": [r\"\\?$\", r\"why \", r\"what \", r\"how \", r\"when \", r\"where \"],\n",
    "    \"share_success\": [r\"i did (it|well)\", r\"i feel proud\", r\"happy to say\"],\n",
    "}\n",
    "\n",
    "\n",
    "EMOTION_PATTERNS = {\n",
    "    \"anxious\": [r\"anxious|nervous|worried|on edge|overthink\"],\n",
    "    \"sad\": [r\"sad|down|depress|worthless|cry\"],\n",
    "    \"angry\": [r\"angry|mad|furious|annoy|irritat\"],\n",
    "    \"lonely\": [r\"alone|lonely|isolated|no one\"],\n",
    "}\n",
    "\n",
    "\n",
    "CRISIS_PATTERNS = {\n",
    "    'self_harm': [r\"kill myself\", r\"suicide\", r\"end my life\", r\"hurt myself\"],\n",
    "    'abuse': [r\"abused\", r\"violence at home\", r\"beaten\", r\"forced\"],\n",
    "    'harm_others': [r\"hurt someone\", r\"kill them\", r\"revenge\"],\n",
    "}\n",
    "\n",
    "\n",
    "def first_match(text, patterns, default=\"neutral\") -> str:\n",
    "    t = text.lower()\n",
    "    for label, pats in patterns.items():\n",
    "        for p in pats:\n",
    "            if re.search(p, t):\n",
    "                return label\n",
    "    return default\n",
    "\n",
    "\n",
    "# synonyms for diversity\n",
    "SYNONYMS = {\n",
    "    'exam': ['test', 'assessment', 'paper'],\n",
    "    'school': ['class', 'college', 'campus'],\n",
    "    'anxious': ['nervous', 'worried', 'on edge'],\n",
    "    'angry': ['upset', 'frustrated', 'mad'],\n",
    "    'sad': ['down', 'blue', 'low'],\n",
    "    'friend': ['peer', 'classmate', 'buddy'],\n",
    "    'parents': ['family', 'mom and dad', 'guardians'],\n",
    "}\n",
    "\n",
    "\n",
    "def simple_augment(text, p=0.25, max_replacement=2) -> str:\n",
    "    tokens = re.findall(r\"\\w+|\\W\", text)\n",
    "    replaced = 0\n",
    "    for i, toks in enumerate(tokens):\n",
    "        low = toks.lower()\n",
    "        if low in SYNONYMS and random.random() < p and replaced < max_replacement:\n",
    "            tokens[i] = random.choice(SYNONYMS[low])\n",
    "            replaced += 1\n",
    "    return ''.join(tokens)\n",
    "\n",
    "\n",
    "# load\n",
    "if os.path.exists(CFG.jsonl_path):\n",
    "    df = load_jsonl_messages(CFG.jsonl_path)\n",
    "else:\n",
    "    df = pd.DataFrame([\n",
    "        {\"text\": \"I feel so anxious about my exams.\",\n",
    "            \"response\": \"It's okay to feel this way. Let's try planning small study steps.\"},\n",
    "        {\"text\": \"No one understands me at school.\",\n",
    "            \"response\": \"That can feel isolating. I'm here to listenâ€”want to share more?\"},\n",
    "        {\"text\": \"How do I stop overthinking everything?\",\n",
    "            \"response\": \"We can try a quick grounding exercise. Want to try together?\"},\n",
    "    ])\n",
    "    print(\"df column: \", df.columns)\n",
    "\n",
    "# heuristics label\n",
    "df['intent'] = df['text'].apply(lambda x: first_match(x, INTENT_PATTERNS, default=\"neutral\"))\n",
    "df['emotion'] = df['text'].apply(lambda x: first_match(x, EMOTION_PATTERNS, default=\"neutral\"))\n",
    "\n",
    "# label maps\n",
    "intent_list = list(dict.fromkeys(list(CFG.intents) + sorted(df['intent'].unique().tolist())))\n",
    "emotion_list = list(dict.fromkeys(list(CFG.emotions) + sorted(df['emotion'].unique().tolist())))\n",
    "\n",
    "intent_label2id = {l:i for i, l in enumerate(intent_list)}\n",
    "intent_id2label = {l:i for i, l in intent_label2id.items()}\n",
    "emotion_label2id = {l:i for i, l in enumerate(emotion_list)}\n",
    "emotion_id2label = {l:i for i, l in emotion_label2id.items()}\n",
    "\n",
    "# numeric labels\n",
    "df['intent_label'] = df['intent'].map(intent_label2id)\n",
    "df['emotion_label'] = df['emotion'].map(emotion_label2id)\n",
    "\n",
    "print(\"Sample: \")\n",
    "print(df.head(5))\n",
    "print(\"\\nIntent labels: \", intent_id2label)\n",
    "print(\"\\nEmotion labels: \", emotion_id2label)\n",
    "\n",
    "#split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df[['intent_label','emotion_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "616f2acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_23718/1898675590.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_intent = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 00:24, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent eval:  {'eval_loss': 0.0002452132466714829, 'eval_runtime': 0.1236, 'eval_samples_per_second': 970.808, 'eval_steps_per_second': 121.351, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_23718/1898675590.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_emotion = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 00:29, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion eval:  {'eval_loss': 0.0004573689366225153, 'eval_runtime': 0.1323, 'eval_samples_per_second': 907.025, 'eval_steps_per_second': 113.378, 'epoch': 4.0}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pred\n\u001b[32m     91\u001b[39m y_true_intent = [\u001b[38;5;28mint\u001b[39m(test_df.iloc[i][\u001b[33m'\u001b[39m\u001b[33mintent_label\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     92\u001b[39m                  \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_df))]\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m y_pred_intent = \u001b[43mpredict_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_intent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf_intent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m y_true_emotion = [\u001b[38;5;28mint\u001b[39m(test_df.iloc[i][\u001b[33m'\u001b[39m\u001b[33memotion_label\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     95\u001b[39m                   \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_df))]\n\u001b[32m     96\u001b[39m y_pred_emotion = predict_labels(test_emotion, clf_emotion)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mpredict_labels\u001b[39m\u001b[34m(ds, model)\u001b[39m\n\u001b[32m     85\u001b[39m     item = ds[i]\n\u001b[32m     86\u001b[39m     inputs = {k: v.unsqueeze(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m item.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m     87\u001b[39m         \u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m.logits\n\u001b[32m     89\u001b[39m     pred.append(\u001b[38;5;28mint\u001b[39m(logits.argmax(dim=\u001b[32m1\u001b[39m).item()))\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pred\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1488\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1480\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1481\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1482\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1483\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1484\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1485\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1486\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1488\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1495\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1498\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1500\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1502\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:942\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    939\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    940\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m embedding_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    951\u001b[39m     attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:180\u001b[39m, in \u001b[36mBertEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[39m\n\u001b[32m    177\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=\u001b[38;5;28mself\u001b[39m.position_ids.device)\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     inputs_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m token_type_embeddings = \u001b[38;5;28mself\u001b[39m.token_type_embeddings(token_type_ids)\n\u001b[32m    183\u001b[39m embeddings = inputs_embeds + token_type_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/torch/nn/modules/sparse.py:192\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Model_Tuning/env/lib/python3.13/site-packages/torch/nn/functional.py:2546\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2540\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2541\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2542\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2543\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2545\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2546\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "clf_tokenizer = AutoTokenizer.from_pretrained(CFG.clf_model_name)\n",
    "\n",
    "\n",
    "class ClfDataset(TorchDataset):\n",
    "    def __init__(self, dataframe, label_col):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.label_col = label_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        enc = clf_tokenizer(\n",
    "            row['text'],\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=256,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item['labels'] = torch.tensor(\n",
    "            int(row[self.label_col]), dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "\n",
    "train_intent = ClfDataset(train_df, 'intent_label')\n",
    "train_emotion = ClfDataset(train_df, 'emotion_label')\n",
    "test_intent = ClfDataset(test_df, 'intent_label')\n",
    "test_emotion = ClfDataset(test_df, 'emotion_label')\n",
    "\n",
    "data_collator = DataCollatorWithPadding(clf_tokenizer)\n",
    "\n",
    "clf_args = TrainingArguments(\n",
    "    output_dir=os.path.join(CFG.work_dir, 'clf'),\n",
    "    per_device_train_batch_size=CFG.batch_size,\n",
    "    per_device_eval_batch_size=CFG.batch_size,\n",
    "    num_train_epochs=CFG.clf_epochs,\n",
    "    logging_steps=50,\n",
    "    learning_rate=CFG.lr,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# intent classifier\n",
    "clf_intent = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CFG.clf_model_name, num_labels=len(intent_label2id)\n",
    ")\n",
    "\n",
    "trainer_intent = Trainer(\n",
    "    model=clf_intent,\n",
    "    args=clf_args,\n",
    "    train_dataset=train_intent,\n",
    "    eval_dataset=test_intent,\n",
    "    tokenizer=clf_tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer_intent.train()\n",
    "intent_eval = trainer_intent.evaluate()\n",
    "print(\"Intent eval: \", intent_eval)\n",
    "\n",
    "# emotion classifier\n",
    "clf_emotion = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CFG.clf_model_name, num_labels=len(emotion_label2id)\n",
    ")\n",
    "\n",
    "trainer_emotion = Trainer(\n",
    "    model=clf_intent,\n",
    "    args=clf_args,\n",
    "    train_dataset=train_emotion,\n",
    "    eval_dataset=test_emotion,\n",
    "    tokenizer=clf_tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer_emotion.train()\n",
    "emotion_eval = trainer_emotion.evaluate()\n",
    "print(\"Emotion eval: \", emotion_eval)\n",
    "\n",
    "# prediction\n",
    "with torch.no_grad():\n",
    "    def predict_labels(ds, model) -> List[int]:\n",
    "        pred = []\n",
    "        for i in range(len(ds)):\n",
    "            item = ds[i]\n",
    "            inputs = {k: v.unsqueeze(0) for k, v in item.items() if k in [\n",
    "                'input_ids', 'attention_mask', 'token_type_ids'] and v is not None}\n",
    "            logits = model(**inputs).logits\n",
    "            pred.append(int(logits.argmax(dim=1).item()))\n",
    "        return pred\n",
    "    y_true_intent = [int(test_df.iloc[i]['intent_label'])\n",
    "                     for i in range(len(test_df))]\n",
    "    y_pred_intent = predict_labels(test_intent, clf_intent)\n",
    "    y_true_emotion = [int(test_df.iloc[i]['emotion_label'])\n",
    "                      for i in range(len(test_df))]\n",
    "    y_pred_emotion = predict_labels(test_emotion, clf_emotion)\n",
    "\n",
    "print(\"\\nIntent classification report:\\n\", classification_report(y_true_intent, y_pred_intent,\n",
    "      target_names=[intent_id2label[i] for i in range(len(intent_id2label))], digits=3))\n",
    "print(\"\\nEmotion classification report:\\n\", classification_report(y_true_emotion, y_pred_emotion,\n",
    "      target_names=[emotion_id2label[i] for i in range(len(emotion_id2label))], digits=3))\n",
    "\n",
    "\n",
    "print(\"Intent confusion matrix:\\n\",\n",
    "      confusion_matrix(y_true_intent, y_pred_intent))\n",
    "print(\"Emotion confusion matrix:\\n\", confusion_matrix(\n",
    "    y_true_emotion, y_pred_emotion))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
